# Job
job.factory.class=org.apache.samza.job.yarn.YarnJobFactory
job.name=__env__.mvc-processor
job.container.count=__mvc_search_indexer_container_count__


# YARN
yarn.package.path=http://__yarn_host__:__yarn_port__/__env__/${project.artifactId}-${pom.version}-distribution.tar.gz

# Metrics
metrics.reporters=snapshot,jmx
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.__env__.metrics
metrics.reporter.jmx.class=org.apache.samza.metrics.reporter.JmxReporterFactory

# Task
task.class=org.ekstep.jobs.samza.task.CompositeSearchIndexerTask
task.inputs=kafka.__env__.mvc.processor.job.request
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.system=kafka
task.checkpoint.replication.factor=__samza_checkpoint_replication_factor__
task.commit.ms=60000
task.window.ms=300000
task.opts=-Dfile.encoding=UTF8
task.broadcast.inputs=kafka.__env__.system.command#0

# Serializers
serializers.registry.json.class=org.ekstep.jobs.samza.serializers.EkstepJsonSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory

# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=json
systems.kafka.streams.metrics.samza.msg.serde=metrics
systems.kafka.consumer.zookeeper.connect=__zookeepers__
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.samza.offset.default=oldest
systems.kafka.producer.bootstrap.servers=__kafka_brokers__

# Job Coordinator
job.coordinator.system=kafka
# Normally, this would be 3, but we have only one broker.
job.coordinator.replication.factor=__samza_coordinator_replication_factor__

# Job specific config properties
search.es_conn_info=__search_es_host__
platform-api-url=__lp_url__
ekstepPlatformApiUserId=ilimi

# neo4j configurations
redis.host=__redis_host__
redis.port=__redis_port__
redis.maxConnections=128
akka.request_timeout=30
environment.id=__environment_id__
graph.passport.key.base=__graph_passport_key__
route.domain=__lp_bolt_url__
route.bolt.read.domain=__lp_bolt_read_url__
route.bolt.write.domain=__lp_bolt_write_url__
route.all=__other_bolt_url__
route.bolt.read.all=__other_bolt_read_url__
route.bolt.write.all=__other_bolt_write_url__
shard.id=__mw_shard_id__
graph.dir="/data/graphDB"
graph.ids=domain,language,en,hi,ka,te,ta
platform.auth.check.enabled=false
platform.cache.ttl=3600000

# Metrics
output.metrics.job.name=mvc-processor
output.metrics.topic.name=__env__.pipeline_metrics

# Nested Fields
nested.fields=badgeAssertions,targets,badgeAssociations,plugins,me_totalTimeSpent,me_totalPlaySessionCount,me_totalTimeSpentInSec,batches

#Failed Topic Config
output.failed.events.topic.name=__env__.learning.events.failed

telemetry_env=__env_name__
installation.id=__installation_id__

# Configuration for default channel ID
channel.default=in.ekstep

# Definition update window
definitions.update.window.ms=300000

mvcsearch.index.name=__mvcsearch_index_name__

# Filter Metadata based on Definition while indexing into ES.
#restrict.metadata.objectTypes=Content,ContentImage,AssessmentItem,Channel,Framework,Category,CategoryInstance,Term,Concept,Dimension,Domain

kafka.topic.system.command=__env__.system.command

