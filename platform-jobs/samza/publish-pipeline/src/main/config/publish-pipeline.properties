# Job
job.factory.class=org.apache.samza.job.yarn.YarnJobFactory
job.name=__env__.publish-pipeline
job.container.count=__publish_pipeline_container_count__

# YARN
yarn.package.path=http://__yarn_host__:__yarn_port__/__env__/${project.artifactId}-${pom.version}-distribution.tar.gz
yarn.container.memory.mb=__yarn_container_memory_mb__

# Metrics
metrics.reporters=snapshot,jmx
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.__env__.metrics
metrics.reporter.jmx.class=org.apache.samza.metrics.reporter.JmxReporterFactory

# Task
task.class=org.ekstep.jobs.samza.task.PublishPipelineTask
task.inputs=kafka.__env__.learning.job.request
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.system=kafka
task.checkpoint.replication.factor=__samza_checkpoint_replication_factor__
task.commit.ms=60000
task.window.ms=300000
task.opts=-Dfile.encoding=UTF8
task.broadcast.inputs=kafka.__env__.system.command#0

# Serializers
serializers.registry.json.class=org.ekstep.jobs.samza.serializers.EkstepJsonSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory

# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=json
systems.kafka.streams.metrics.samza.msg.serde=metrics
systems.kafka.consumer.zookeeper.connect=__zookeepers__
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.samza.offset.default=oldest
systems.kafka.producer.bootstrap.servers=__kafka_brokers__


# Job Coordinator
job.coordinator.system=kafka
# Normally, this would be 3, but we have only one broker.
job.coordinator.replication.factor=__samza_coordinator_replication_factor__

#Job specif configuration
redis.host=__redis_host__
redis.port=__redis_port__
redis.maxConnections=128
akka.request_timeout=600
environment.id=__environment_id__
graph.passport.key.base=__graph_passport_key__
route.domain=__lp_bolt_url__
route.bolt.read.domain=__lp_bolt_read_url__
route.bolt.write.domain=__lp_bolt_write_url__
route.all=__other_bolt_url__
route.bolt.read.all=__other_bolt_read_url__
route.bolt.write.all=__other_bolt_write_url__
shard.id=__mw_shard_id__

content.keyspace.name=__keyspace_name__
content.keyspace.table=__keyspace_table__
assessment.keyspace.name=__keyspace_name__
hierarchy.keyspace.name=__hierarchy_keyspace_name__
content.hierarchy.table=content_hierarchy
CONTENT_TO_VEC_URL=__content_to_vec_url__
platform-api-url=__lp_url__
ekstepPlatformApiUserId=ilimi
graph.dir="/data/graphDB"
graph.ids=["domain","language","en","hi","ka","te","ta"]
platform.auth.check.enabled=false
platform.cache.ttl=3600000
kafka.topics.backend.telemetry=__env__.telemetry.raw
kafka.topics.failed=__env__.learning.job.request

#Current environment
cloud_storage.env=__cloud_storage_config_environment__

#Folder configuration
cloud_storage.content.folder=content
cloud_storage.itemset.folder=itemset
cloud_storage.asset.folder=assets
cloud_storage.artefact.folder=artifact
cloud_storage.bundle.folder=bundle
cloud_storage.media.folder=media
cloud_storage.ecar.folder=ecar_files
cloud_storage.upload.url.ttl=600


# Media download configuration
content.media.base.url=__content_media_base_url__
plugin.media.base.url=__plugin_media_base_url__

#directory location where store unzip file
dist.directory=/tmp/dist/
output.zipfile=/tmp/story.zip
source.folder=/tmp/temp2/
save.directory=/tmp/temp/

MAX_CONTENT_PACKAGE_FILE_SIZE_LIMIT=52428800
MAX_ASSET_FILE_SIZE_LIMIT=20971520
RETRY_ASSET_DOWNLOAD_COUNT=1

lp.tempfile.location=__lp_tmpfile_location__
max.iteration.count.samza.job=__max_iteration_count_for_samza_job__
publish.content.limit=200


# Metrics
output.metrics.job.name=publish-pipeline
output.metrics.topic.name=__env__.pipeline_metrics

#Failed Topic Config
output.failed.events.topic.name=__env__.learning.events.failed

telemetry_env=__env_name__
installation.id=__installation_id__

# Cloud store details
cloud_storage_type=__cloud_storage_type__
azure_storage_key=__azure_storage_key__
azure_storage_secret=__azure_storage_secret__
azure_storage_container=__azure_storage_container__
aws_storage_key=__aws_access_key_id__
aws_storage_secret=__aws_secret_access_key__
aws_storage_container=__aws_storage_container__

# Configuration for default channel ID
channel.default=in.ekstep


content.publish.invoke_web_hook=__invoke_web_hook__

#Streamable media type list
stream.mime.type=__streaming_mime_type__
stream.keyspace.name=__env___platform_db
stream.keyspace.table=job_request

cassandra.lp.connection=__cassandra_lp_connection__
cassandra.lpa.connection=__cassandra_lpa_connection__

#restrict.metadata.objectTypes=Content,ContentImage

kafka.topic.system.command=__env__.system.command

# Consistency Level for Multi Node Cassandra cluster
cassandra.lp.consistency.level=QUORUM

compositesearch.index.name=__compositesearch_index_name__

content.nested.fields=badgeAssertions,targets,badgeAssociations

search.es_conn_info=__search_es_host__

# Content Tagging Config for Backward Compatibility in Mobile App

content.cache.read=true
content.cache.hierarchy=true

# Max size(width/height) of thumbnail in pixels
max.thumbnail.size.pixels=150

#Post publish Job topic name
post.publish.event.topic=__env__.content.postpublish.request

# Print service config
#kp.print.service.base.url=__kp_print_service_base_url__
kp.print.service.base.url=__kp_print_service_base_url__
lp.assessment.tmp_file_location=/tmp/
lp.assessment.template_name=questionSetTemplate.vm

# Content Tagging Config for Backward Compatibility in Mobile App
content.tagging.backward_enable=true
content.tagging.property=subject,medium