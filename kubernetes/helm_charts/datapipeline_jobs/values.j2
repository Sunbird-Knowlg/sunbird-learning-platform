namespace: {{ flink_namespace }}
imagepullsecrets: {{ imagepullsecrets }}
dockerhub: {{ dockerhub }}
repository: {{flink_repository|default('knowledge-platform-jobs')}}
image_tag: {{ image_tag }}
azure_account: {{ azure_account }}
azure_secret: {{ azure_secret }}

replicaCount: {{taskmana_replicacount|default(1)}}

jobmanager:
  rpc_port: {{ jobmanager_rpc_port }}
  blob_port: {{ jobmanager_blob_port }}
  query_port: {{ jobmanager_query_port }}
  ui_port: {{ jobmanager_ui_port }}
  prom_port: {{ jobmanager_prom_port }}
  heap_memory: {{ jobmanager_heap_memory }}

service: {{ jobmanager_ui_service|to_json }}

rest_port: {{ jobmanager_ui_rest_port }}
resttcp_port: {{ jobmanager_ui_tcp_port }}

taskmanager:
  prom_port: {{ taskmanager_prom_port }}
  rpc_port: {{ taskmanager_rpc_port }}
  heap_memory: {{ taskmanager_heap_memory }}
  replicas: {{taskmanager_replicacount|default(1)}}

job_classname: {{ job_classname }}
{{ taskmanager_liveness | to_nice_yaml }}


base_config: |
  kafka {
      broker-servers = "{{ kafka_brokers }}"
      zookeeper = "{{ zookeepers }}"
      producer {
        max-request-size = {{ producer_max_request_size }}
      }
    }
    job {
      env = "{{ env_name }}"
      enable.distributed.checkpointing = true
      statebackend {
        blob {
          storage {
            account = "{{ azure_account }}.blob.core.windows.net"
            container = "{{ flink_container_name }}"
            checkpointing.dir = "checkpoint"
          }
        }
        base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
      }
    }
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = {{ checkpoint_compression_enabled|lower }}
      checkpointing.interval = {{ checkpoint_interval }}
      checkpointing.pause.between.seconds = {{ checkpoint_pause_between_seconds }}
      restart-strategy.attempts = {{ restart_attempts }}
      restart-strategy.delay = {{ restart_delay }} # in milli-seconds
    }
    redis {
      host = {{ dp_redis_host }}
      port = 6379
    }
    lms-cassandra {
      host = "{{ core_cassandra_connection_ip }}"
      port = "9042"
    }
    neo4j {
      routePath = "{{ neo4j_route_path }}"
      graph = "domain"
    }
    es {
        basePath = "{{ search_es_host }}"
    }
    schema {
      basePath = "{{ kp_schema_base_path }}"
      supportedVersion = {
        itemset = "2.0"
      }
    }

activity-aggregate-updater:
  activity-aggregate-updater: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.coursebatch.job.request
      output.audit.topic = {{ env_name }}.telemetry.raw
      output.failed.topic = {{ env_name }}.activity.agg.failed
      output.certissue.topic = {{ env_name }}.issue.certificate.request
      groupId = {{ env_name }}-activity-aggregate-group
    }
    task {
      window.shards = {{ activity_agg_window_shards }}
      checkpointing.interval = {{ activity_agg_checkpointing_interval }}
      checkpointing.pause.between.seconds = {{ activity_agg_checkpointing_pause_interval }}
      restart-strategy.attempts = {{ restart_attempts }} # max 3 restart attempts
      restart-strategy.delay = 240000 # in milli-seconds # on max restarts (3) within 4 min the job will fail.
      consumer.parallelism = {{ activity_agg_consumer_parallelism }}
      dedup.parallelism = {{ activity_agg_dedup_parallelism }}
      activity.agg.parallelism = {{ activity_agg_parallelism }}
      enrolment.complete.parallelism = {{ enrolment_complete_parallelism }}
    }
    lms-cassandra {
      keyspace = "{{ middleware_course_keyspace }}"
      consumption.table = "{{ middleware_consumption_table }}"
      user_activity_agg.table = "{{ middleware_user_activity_agg_table }}"
      user_enrolments.table = "user_enrolments"
    }
    redis {
      database {
        relationCache.id = 10
      }
    }
    dedup-redis {
      host = {{ dedup_redis_host }}
      port = 6379
      database.index = {{ activity_agg_dedup_index }}
      database.expiry = {{ activity_agg_dedup_expiry }}
    }
    threshold.batch.read.interval = {{ activity_agg_batch_interval }}
    threshold.batch.read.size = {{ activity_agg_batch_read_size }}
    threshold.batch.write.size = {{ activity_agg_batch_write_size }}
    activity {
      module.aggs.enabled = true
      input.dedup.enabled = true
      filter.processed.enrolments = {{ activity_agg_enrolment_filter_processe_enabled }}
      collection.status.cache.expiry = {{ activity_agg_collection_status_cache_expiry_time }}
    }


  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['activity-aggregate-updater'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['activity-aggregate-updater'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['activity-aggregate-updater'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

relation-cache-updater:
  relation-cache-updater: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.content.postpublish.request
      groupId = {{ env_name }}-relation-cache-updater-group
    }
    task {
      consumer.parallelism = {{ relation_cache_updater_consumer_parallelism }}
      parallelism = {{ relation_cache_updater_parallelism }}
    }
    lms-cassandra {
          keyspace = "{{ middleware_hierarchy_keyspace }}"
          table = "{{ middleware_content_hierarchy_table }}"
    }
    redis {
      database.index = 10
    }
    dp-redis {
      host = {{ dp_redis_host }}
      port = 6379
      database.index = 5
    }

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['relation-cache-updater'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['relation-cache-updater'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['relation-cache-updater'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

post-publish-processor:
  post-publish-processor: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.content.postpublish.request
      groupId = {{ env_name }}-post-publish-processor-group
      publish.topic = {{ env_name }}.learning.job.request
      qrimage.topic = {{ env_name }}.qrimage.request
    }
    task {
      consumer.parallelism = {{ post_publish_processor_consumer_parallelism }}
      router.parallelism = {{ post_publish_event_router_parallelism }}
      shallow_copy.parallelism = {{ post_publish_shallow_copy_parallelism }}
      link_dialcode.parallelism = {{ post_publish_link_dialcode_parallelism }}
      batch_create.parallelism = {{ post_publish_batch_create_parallelism }}
    }
    lms-cassandra {
      keyspace = "{{ middleware_course_keyspace }}"
      batchTable = "course_batch"
    }
    dialcode-cassandra {
      keyspace = "dialcodes"
      imageTable = "dialcode_images"
    }
    service {
      search.basePath = "{{ kp_search_service_base_url }}"
      lms.basePath = "{{ lms_service_base_url }}"
      learning_service.basePath = "{{ kp_learning_service_base_url }}"
      dial.basePath = "https://{{domain_name}}/dial/"
    }

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['post-publish-processor'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['post-publish-processor'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['post-publish-processor'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

questionset-publish:
  questionset-publish: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.assessment.publish.request
      post_publish.topic = {{ env_name }}.content.postpublish.request
      groupId = {{ env_name }}-questionset-publish-group
    }
    task {
      consumer.parallelism = 1
      parallelism = 1
      router.parallelism = 1
    }
    question {
      keyspace = "{{ assessment_keyspace_name }}"
      table = "question_data"
    }
    questionset {
      keyspace = "{{ hierarchy_keyspace_name }}"
      table = "questionset_hierarchy"
    }
    print_service.base_url = "{{ kp_print_service_base_url }}"
    cloud_storage_type="{{ cloud_store }}"
    azure_storage_key="{{ sunbird_public_storage_account_name }}"
    azure_storage_secret="{{ sunbird_public_storage_account_key }}"
    azure_storage_container="{{ azure_public_container }}"

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['questionset-publish'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['questionset-publish'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['questionset-publish'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

content-publish:
  content-publish: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.publish.job.request
      post_publish.topic = {{ env_name }}.content.postpublish.request
      groupId = {{ env_name }}-content-publish-group
      topic.system.command = {{ env_name }}.system.command
    }
    task {
      consumer.parallelism = 1
      parallelism = 1
      router.parallelism = 1
    }
    
    cloud_storage_type="{{ cloud_store }}"
    azure_storage_key="{{ sunbird_public_storage_account_name }}"
    azure_storage_secret="{{ sunbird_public_storage_account_key }}"
    azure_storage_container="{{ azure_public_container }}"

    # Content Pblish specific data

    content {
      keyspace.name = "{{ content_keyspace_name }}"
      data.table = "content_data"
      hierarchy.table = "content_hierarchy"
      media.base.url = "{{ content_media_base_url }}"
      nested.fields = "badgeAssertions,targets,badgeAssociations"
      cache.read = true
      cache.hierarchy = true
      tagging.backward_enable=true
      tagging.property="subject,medium"
      upload.context.driven=true
    }
    redis {
      host = "{{ redis_host }}"
      port = {{ redis_port }}
      maxConnections = 128
    }
    akka {
      request_timeout = 600
    }
    environment {
      id = "{{ environment_id }}"
    }
    graph {
      passport.key.base = "{{ graph_passport_key }}"
    }
    route {
      domain = "{{ lp_bolt_url }}"
      bolt.read.domain = "{{ lp_bolt_read_url }}"
      bolt.write.domain = "{{ lp_bolt_write_url }}"
      all = "{{ other_bolt_url }}"
      bolt.read.all = " {{ other_bolt_read_url }}"
      bolt.write.all = "{{ other_bolt_write_url }}"
    }
    shard {
      id = 1
    }
    assessment{
      keyspace.name = "{{ content_keyspace_name }}"
    }
    hierarchy{
      keyspace.name = "{{ hierarchy_keyspace_name }}"
    }
    graph {
      dir="/data/graphDB"
      ids = ["domain","language","en","hi","ka","te","ta"]
    }
    platform {
      cache.ttl = "3600000"
    }
    cloud_storage {
      env = "{{ cloud_storage_config_environment }}"
      content.folder = "content"
      itemset.folder = "itemset"
      asset.folder = "assets"
      artefact.folder = "artifact"
      bundle.folder = "bundle"
      media.folder = "media"
      ecar.folder = "ecar_files"
      upload.url.ttl = "600"
    }
    plugin {
      media.base.url = "{{ plugin_media_base_url }}"
    }

    stream {
      mime.type = "video/mp4,video/webm"
      keyspace.name = "{{ env }}_platform_db"
      keyspace.table = "job_request"
    }

    cassandra {
      lp.connection = "{{ lp_cassandra_connection }}"
      lpa.connection = "{{ dp_cassandra_connection  }}"
      lp.consistency.level = "QUORUM"
    }

    platform-api-url="{{ lp_url }}"
    ekstepPlatformApiUserId="sunbird"

    dist.directory="/tmp/dist/"
    output.zipfile="/tmp/story.zip"
    source.folder="/tmp/temp2/"
    save.directory="/tmp/temp/"
  
    MAX_CONTENT_PACKAGE_FILE_SIZE_LIMIT=52428800
    MAX_ASSET_FILE_SIZE_LIMIT=20971520
    RETRY_ASSET_DOWNLOAD_COUNT=1

    lp.tempfile.location="{{ lp_tmpfile_location }}"
    max.iteration.count.samza.job=2
    publish.content.limit=200

    telemetry_env="{{ env_name }}"
    installation.id="{{ instance_name }}"

    # Configuration for default channel ID
    channel.default="in.ekstep"

    compositesearch.index.name="{{ compositesearch_index_name }}"
    search.es_conn_info="{{ search_es_host }}"

    max.thumbnail.size.pixels=150

    kp.print.service.base.url="{{ kp_print_service_base_url }}"
    lp.assessment.tmp_file_location="/tmp/"
    lp.assessment.template_name="questionSetTemplate.vm"

    itemset.generate.pdf={{ itemset_generate_pdf }}
    content.streaming_enabled={{ content_streaming_enabled }}

    #Configuration added to handle large artifacts
    content.artifact.size.for_online=209715200

    #Content Type Primary Category mapping
    contentTypeToPrimaryCategory="{\"ClassroomTeachingVideo\":\"Explanation Content\",\"ConceptMap\":\"Learning Resource\",\"Course\":\"Course\",\"CuriosityQuestionSet\":\"Practice Question Set\",\"eTextBook\":\"eTextbook\",\"ExperientialResource\":\"Learning Resource\",\"ExplanationResource\":\"Explanation Content\",\"ExplanationVideo\":\"Explanation Content\",\"FocusSpot\":\"Teacher Resource\",\"LearningOutcomeDefinition\":\"Teacher Resource\",\"MarkingSchemeRubric\":\"Teacher Resource\",\"PedagogyFlow\":\"Teacher Resource\",\"PracticeQuestionSet\":\"Practice Question Set\",\"PracticeResource\":\"Practice Question Set\",\"SelfAssess\":\"Course Assessment\",\"TeachingMethod\":\"Teacher Resource\",\"TextBook\":\"Digital Textbook\",\"Collection\":\"Content Playlist\",\"ExplanationReadingMaterial\":\"Learning Resource\",\"LearningActivity\":\"Learning Resource\",\"LessonPlan\":\"Content Playlist\",\"LessonPlanResource\":\"Teacher Resource\",\"PreviousBoardExamPapers\":\"Learning Resource\",\"TVLesson\":\"Explanation Content\",\"OnboardingResource\":\"Learning Resource\",\"ReadingMaterial\":\"Learning Resource\",\"Template\":\"Template\",\"Asset\":\"Asset\",\"Plugin\":\"Plugin\",\"LessonPlanUnit\":\"Lesson Plan Unit\",\"CourseUnit\":\"Course Unit\",\"TextBookUnit\":\"Textbook Unit\"}"

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['content-publish'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['content-publish'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['content-publish'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

video-stream-generator:
  video-stream-generator: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = "{{ env_name }}.content.postpublish.request"
      groupId = "{{ env_name }}-video-stream-generator-group"
    }
    task {
      window.shards = {{ activity_agg_window_shards }}
      timer.duration = 60
      consumer.parallelism = {{ vide_stream_generator_consumer_parallelism }}
      parallelism = {{ vide_stream_generator_parallelism }}
    }
    dp-cassandra {
      keyspace = "{{ env_name }}_platform_db"
      table = "job_request"
      host = {{ dp_cassandra_connection_ip }}
      port = "9042"
    }
    threshold.batch.read.interval = {{ activity_agg_batch_interval }}
    threshold.batch.read.size = {{ activity_agg_batch_read_size }}
    threshold.batch.write.size = {{ activity_agg_batch_write_size }}
    service.learning.basePath="{{lp_url}}"
    azure {
      location = "centralindia"
      login {
        endpoint="https://login.microsoftonline.com"
      }
      api {
        endpoint="https://management.azure.com"
        version = "2018-07-01"
      }
      transform {
        default = "media_transform_default"
        hls = "media_transform_hls"
      }
      stream {
        base_url = "https://sunbirdspikemedia-inct.streaming.media.azure.net"
        endpoint_name = "default"
        protocol = "Hls"
        policy_name = "Predefined_ClearStreamingOnly"
      }
    }
    azure_tenant="{{ azure_tenant }}"
    azure_subscription_id="{{ azure_subscription_id }}"
    azure_account_name="{{ azure_account_name }}"
    azure_resource_group_name="{{ azure_resource_group_name }}"
    azure_token_client_key="{{ azure_token_client_key }}"
    azure_token_client_secret="{{ azure_token_client_secret }}"


  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['video-stream-generator'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['video-stream-generator'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['video-stream-generator'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1



search-indexer:
  search-indexer: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = "{{ env_name }}.learning.graph.events"
      error.topic = "{{ env_name }}.learning.events.failed"
      groupId = "{{ env_name }}-search-indexer-group"
    }
    task {
      consumer.parallelism = {{ search_indexer_consumer_parallelism }}
      router.parallelism = {{ transaction_event_router_parallelism }}
      compositeSearch.parallelism = {{ composite_search_indexer_parallelism }}
      dialcodeIndexer.parallelism = {{ dialcode_external_indexer_parallelism }}
      dialcodemetricsIndexer.parallelism = {{ dialcode_metric_indexer_parallelism }}
    }
    compositesearch.index.name = "compositesearch"
    dialcode.index.name = "dialcode"
    dailcodemetrics.index.name = "dialcodemetrics"
    restrict.metadata.objectTypes = []
    nested.fields = ["badgeAssertions", "targets", "badgeAssociations", "plugins", "me_totalTimeSpent", "me_totalPlaySessionCount", "me_totalTimeSpentInSec", "batches", "trackable", "credentials"]
    schema.definition_cache.expiry = {{ schema_definition_cache_expiry_in_sec }}

  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['search-indexer'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['search-indexer'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['search-indexer'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1

enrolment-reconciliation:
  enrolment-reconciliation: |+
    include file("/data/flink/conf/base-config.conf")
    kafka {
      input.topic = {{ env_name }}.batch.enrolment.sync.request
      output.audit.topic = {{ env_name }}.telemetry.raw
      output.failed.topic = {{ env_name }}.activity.agg.failed
      output.certissue.topic = {{ env_name }}.issue.certificate.request
      groupId = {{ env_name }}-enrolment-reconciliation-group
    }
    task {
      restart-strategy.attempts = {{ restart_attempts }} # max 3 restart attempts
      restart-strategy.delay = 240000 # in milli-seconds # on max restarts (3) within 4 min the job will fail.
      consumer.parallelism = {{ enrolment_reconciliation_consumer_parallelism }}
      enrolment.reconciliation.parallelism = {{ enrolment_reconciliation_parallelism }}
      enrolment.complete.parallelism = {{ enrolment_complete_parallelism }}
    }
    lms-cassandra {
      keyspace = "{{ middleware_course_keyspace }}"
      consumption.table = "{{ middleware_consumption_table }}"
      user_activity_agg.table = "{{ middleware_user_activity_agg_table }}"
      user_enrolments.table = "user_enrolments"
    }
    redis {
      database {
        relationCache.id = 10
      }
    }
    threshold.batch.write.size = {{ enrolment_reconciliation_batch_write_size }}
    activity {
      module.aggs.enabled = true
      collection.status.cache.expiry = {{ enrolment_reconciliation_collection_status_cache_expiry_time }}
    }
    service {
      search.basePath = "{{ kp_search_service_base_url }}"
    }


  flink-conf: |+
    jobmanager.memory.flink.size: {{ flink_job_names['enrolment-reconciliation'].jobmanager_memory }}
    taskmanager.memory.flink.size: {{ flink_job_names['enrolment-reconciliation'].taskmanager_memory }}
    taskmanager.numberOfTaskSlots: {{ flink_job_names['enrolment-reconciliation'].taskslots }}
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
